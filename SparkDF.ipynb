{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#apache-arrow-in-spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext()\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json object\n",
    "#with open(\"data/json_data.json\", encoding=\"utf-8\") as f:\n",
    "#    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json object testData\n",
    "# load json object\n",
    "with open(\"data/testData.json\", encoding=\"utf-8\") as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe; each row is a tweet, each column is a tweet attribute + tweet ID\n",
    "pdf = json_normalize(data=d['tweets'], record_path='tweet',\n",
    "                            meta=['tweet_id'])\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________Pandas Text Cleaning__________________\n",
    "# Convert to lowercase\n",
    "pdf['tweet_text'] = pdf['tweet_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "pdf['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx = '[.,]'  # remove , or .\n",
    "pdf['tweet_text'] = pdf['tweet_text'].str.replace(rgx, '')\n",
    "pdf['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count numbers of hastags used in each tweet.\n",
    "pdf['hastags'] = pdf['tweet_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "pdf[['tweet_text','hastags']].head()\n",
    "pdf['hastags'].max()\n",
    "pdf[\"hastags\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by keyword, we can have it search many keywords such as [roadblocking, landslide, poweroutage...]\n",
    "#Which ever we find most suitable.\n",
    "keyword = \"help\"\n",
    "pdf['keyword'] = pdf['tweet_text'].apply(lambda x: len([x for x in x.split() if x.startswith(keyword)]))\n",
    "pdf[['keyword']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"lat\"] = pdf[\"tweet_location\"].apply(lambda x: x[0])\n",
    "pdf[\"long\"] = pdf[\"tweet_location\"].apply(lambda x: x[1])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordDf = pdf[(pdf['keyword'] >= 1)]\n",
    "keywordDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkwdf = keywordDf[['lat','long']].astype(float)\n",
    "lkwdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import *\n",
    "#mySchema = StructType([StructField(\"lat\", FloatType(), True),\n",
    "                      #StructField(\"long\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "#df = spark.createDataFrame(pdf)\n",
    "#df = spark.createDataFrame(keywordDf, schema = mySchema)\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"lat\",\"long\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark k-mean over locations to localize an event area.\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locData = df.select(\"lat\",\"long\")\n",
    "#locData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/47585723/kmeans-clustering-in-pyspark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=16, seed=1)  # 2 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.select(\"tweet_id\",\"prediction\",\"lat\",\"long\",\"tweet_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the cluster centers. (\"Centroids\")\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for idx,center in enumerate(centers):\n",
    "    print(idx,center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.select(\"prediction\",\"tweet_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF tf-idf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = transformed.groupBy('prediction')\\\n",
    "  .agg(F.collect_list('tweet_text')).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.select(\"collect_list(tweet_text)\").head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/46178325/flatten-nested-struct-in-pyspark-array\n",
    "documents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "#flat_df = documents.select(\"prediction\", \"collect_list(tweet_text).*\")\n",
    "\n",
    "flat_df = documents.select(\"prediction\", *[c + \".*\" for c in \"collect_list(tweet_text)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = transformed.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pandas dataframe to jsone file\n",
    "with open('data/testDataCluster.json', 'w') as f:\n",
    "    f.write(result_pdf.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keyword_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
