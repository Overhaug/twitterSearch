{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true Enable Arrow-based columnar data transfers for speed.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark context and sql context.\n",
    "#sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json object testData\n",
    "with open(\"data/testData.json\", encoding=\"utf-8\") as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe; each row is a tweet, each column is a tweet attribute + tweet ID\n",
    "pdf = json_normalize(data=d['tweets'], record_path='tweet',\n",
    "                            meta=['tweet_id'])\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________Pandas Text Cleaning__________________\n",
    "# Convert to lowercase\n",
    "pdf['tweet_text'] = pdf['tweet_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "pdf['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgx = '[.,]'  # remove , or .\n",
    "pdf['tweet_text'] = pdf['tweet_text'].str.replace(rgx, '')\n",
    "pdf['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count numbers of hastags used in each tweet.\n",
    "pdf['hastags'] = pdf['tweet_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "pdf[['tweet_text','hastags']].head()\n",
    "pdf['hastags'].max()\n",
    "pdf[\"hastags\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by keyword, we can have it search many keywords such as [roadblocking, landslide, poweroutage...]\n",
    "#Which ever we find most suitable.\n",
    "keyword = \"help\"\n",
    "pdf['keyword'] = pdf['tweet_text'].apply(lambda x: len([x for x in x.split() if x.startswith(keyword)]))\n",
    "pdf[['keyword']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split latitude and longitude.\n",
    "pdf[\"lat\"] = pdf[\"tweet_location\"].apply(lambda x: x[0])\n",
    "pdf[\"long\"] = pdf[\"tweet_location\"].apply(lambda x: x[1])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweet that does not cointain any of the keywords.\n",
    "#keywordDf = pdf[(pdf['keyword'] >= 1)]\n",
    "#keywordDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from a Pandas DataFrame\n",
    "df = sqlContext.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"tweet_id\",\"tweet_date\",\"lat\",\"long\",\"tweet_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use latitude and longitide as x and y features with k-means\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.select(\"tweet_id\",\"features\",\"tweet_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run K-means over longitude and latitude, with k amount of target clusters.\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=16, seed=1) # 16 clusters\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tweets with their associated cluster(\"prediction\") \n",
    "transformed = model.transform(new_df)\n",
    "transformed.select(\"tweet_id\",\"prediction\",\"lat\",\"long\",\"tweet_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the cluster centers. (\"Centroids\")\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for idx,center in enumerate(centers):\n",
    "    print(idx,center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines all tweets under its associated k-means cluster class.\n",
    "import pyspark.sql.functions as F\n",
    "documents = transformed.groupBy('prediction')\\\n",
    "  .agg(F.collect_list('tweet_text').alias(\"cluster_text\")).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.select(\"cluster_text\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine each list of tweets to a string.\n",
    "documents = documents.withColumn('cluster_text', F.concat_ws(',', 'cluster_text'))\n",
    "documents.select(\"cluster_text\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame back to a Pandas DataFrame.\n",
    "result_pdf = transformed.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pandas dataframe to jsone file\n",
    "with open('data/testDataCluster.json', 'w') as f:\n",
    "    f.write(result_pdf.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
